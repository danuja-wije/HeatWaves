{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Danuja\n",
      "[nltk_data]     Wijerathna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Danuja\n",
      "[nltk_data]     Wijerathna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'survey_combined -final.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical fields\n",
    "numerical_fields = ['Current Weight (kg)']\n",
    "categorical_fields = ['Gender', 'Diuretics Medication', 'Beta-Blockers Medication', 'Medical Records Consent']\n",
    "text_columns = [\n",
    "    'Typical Workday in Heatwave',\n",
    "    'Determine Water Intake',\n",
    "    'Medication Effect on Hydration',\n",
    "    'Feel When Dehydrated',\n",
    "    'Severe Dehydration Experience',\n",
    "    'Hydration Habits',\n",
    "    'Tools for Monitoring Hydration'\n",
    "]\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = str(text).lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization (implicitly done by CountVectorizer later)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Enhanced extract_topics function with preprocessing\n",
    "def extract_topics(data, n_topics=3, n_features=1000):\n",
    "    # Preprocess the text data\n",
    "    preprocessed_data = data.apply(preprocess_text)\n",
    "\n",
    "    # Vectorization with TF-IDF to give less weight to more common words\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "    # Apply LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    # Get the topic distribution for each document\n",
    "    topic_distribution = lda.transform(dtm)\n",
    "    return topic_distribution\n",
    "\n",
    "for column in text_columns:\n",
    "    topics = extract_topics(df[column])\n",
    "    for i in range(topics.shape[1]):\n",
    "        X[f'{column}_Topic_{i}'] = topics[:, i]\n",
    "\n",
    "X= X.drop(columns=text_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    48\n",
      "2    29\n",
      "0    23\n",
      "Name: Predicted Body Water Level Combined, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering to the scaled features\n",
    "kmeans_combined = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_combined.fit(X_scaled)  # Use X_scaled directly\n",
    "\n",
    "# Assign the clusters as the predicted body water levels with combined features\n",
    "df['Predicted Body Water Level Combined'] = kmeans_combined.labels_\n",
    "\n",
    "# Check the distribution of predicted body water levels with combined features\n",
    "print(df['Predicted Body Water Level Combined'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv('temp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
